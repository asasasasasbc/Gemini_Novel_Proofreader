# novel_proofreader.py
# pip install -r requirements.txt
import gradio as gr
import google.generativeai as genai
import openai
import yaml
import re
import os
import time
from datetime import datetime

# --- 1. é…ç½®åŠ è½½ (åˆ†ç¦») ---
def load_config(config_file):
    """ä»æŒ‡å®šçš„ yaml æ–‡ä»¶åŠ è½½é…ç½®"""
    print(f"[DEBUG] æ­£åœ¨å°è¯•åŠ è½½ {config_file}...")
    try:
        with open(config_file, "r", encoding="utf-8") as file:
            config = yaml.safe_load(file)
            print(f"[DEBUG] {config_file} åŠ è½½æˆåŠŸã€‚")
            return config
    except FileNotFoundError:
        print(f"[ERROR] é”™è¯¯ï¼šæ‰¾ä¸åˆ° {config_file} æ–‡ä»¶ã€‚")
        raise FileNotFoundError(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ° {config_file} æ–‡ä»¶ã€‚")
    except Exception as e:
        print(f"[ERROR] è¯»å– {config_file} æ—¶å‡ºé”™: {e}")
        raise IOError(f"è¯»å– {config_file} æ—¶å‡ºé”™: {e}")

def load_models():
    """ä» models.yaml åŠ è½½æ¨¡å‹åˆ—è¡¨"""
    return load_config("models.yaml")

# --- 2. API è°ƒç”¨å±‚ (å¢åŠ  temperature å‚æ•°) ---
def proofread_chapter_with_gemini(chapter_title, chapter_content, model_name, api_key, prompt_template, temperature, proxy_url=None): # <--- å¢åŠ  temperature
    """ä½¿ç”¨ Gemini API è¿›è¡Œæ ¡å¯¹ï¼ŒåŒ…å«é‡è¯•é€»è¾‘"""
    if proxy_url:
        os.environ['https_proxy'] = proxy_url
    else:
        if 'https_proxy' in os.environ: del os.environ['https_proxy']

    try:
        # <--- æ–°å¢ï¼šä¸º Gemini é…ç½® Temperature ---
        generation_config = genai.types.GenerationConfig(temperature=temperature)
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(model_name, generation_config=generation_config)
    except Exception as e:
        return f"æ— æ³•é…ç½®æˆ–åˆå§‹åŒ– Gemini æ¨¡å‹: {e}"

    prompt = prompt_template.format(chapter_title=chapter_title, chapter_content=chapter_content)
    max_retries, retry_delay = 2, 3

    try:
        for attempt in range(max_retries + 1):
            try:
                print(f"[DEBUG] [Gemini] æ­£åœ¨ä¸ºç« èŠ‚ '{chapter_title}' è°ƒç”¨ API (æ¨¡å‹: {model_name}, Temp: {temperature}, å°è¯•: {attempt + 1})...") # <--- ä¿®æ”¹æ—¥å¿—
                response = model.generate_content(prompt)
                return response.text
            except Exception as e:
                if attempt >= max_retries: raise e
                print(f"[ERROR] [Gemini] å°è¯• {attempt + 1} å¤±è´¥: {e}. {retry_delay} ç§’åé‡è¯•...")
                time.sleep(retry_delay)
    finally:
        if 'https_proxy' in os.environ: del os.environ['https_proxy']


def proofread_chapter_with_openai(chapter_title, chapter_content, model_name, api_key, prompt_template, temperature, base_url=None): # <--- å¢åŠ  temperature
    """ä½¿ç”¨ OpenAI API è¿›è¡Œæ ¡å¯¹ï¼ŒåŒ…å«é‡è¯•é€»è¾‘"""
    try:
        client = openai.OpenAI(api_key=api_key, base_url=base_url if base_url else None)
    except Exception as e:
        return f"æ— æ³•é…ç½®æˆ–åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯: {e}"

    prompt = prompt_template.format(chapter_title=chapter_title, chapter_content=chapter_content)
    max_retries, retry_delay = 2, 3

    for attempt in range(max_retries + 1):
        try:
            print(f"[DEBUG] [OpenAI] æ­£åœ¨ä¸ºç« èŠ‚ '{chapter_title}' è°ƒç”¨ API (æ¨¡å‹: {model_name}, Temp: {temperature}, å°è¯•: {attempt + 1})...") # <--- ä¿®æ”¹æ—¥å¿—
            response = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature # <--- æ–°å¢ï¼šä¸º OpenAI é…ç½® Temperature
            )
            return response.choices[0].message.content
        except Exception as e:
            if attempt >= max_retries: raise e
            print(f"[ERROR] [OpenAI] å°è¯• {attempt + 1} å¤±è´¥: {e}. {retry_delay} ç§’åé‡è¯•...")
            time.sleep(retry_delay)

# --- 3. ä¸»å¤„ç†å‡½æ•° (å¢åŠ  temperature å‚æ•°) ---
def process_and_proofread_novel(novel_text, api_provider, model_name, prompt_mode, temperature): # <--- å¢åŠ  temperature
    """åˆ†å‰²æ–‡æœ¬ï¼Œæ ¹æ®é€‰æ‹©çš„ API è°ƒåº¦æ ¡å¯¹ä»»åŠ¡ï¼Œå¹¶æµå¼è¿”å›ç»“æœ"""
    print(f"\n--- å¼€å§‹æ–°çš„æ ¡å¯¹ä»»åŠ¡ (API: {api_provider}, Temp: {temperature}) ---") # <--- ä¿®æ”¹æ—¥å¿—
    if not novel_text.strip():
        yield "è¯·è¾“å…¥å°è¯´å†…å®¹ã€‚", gr.File(visible=False)
        return

    try:
        with open(f"prompts/{prompt_mode}.txt", "r", encoding="utf-8") as f: prompt_template = f.read()
        
        if api_provider == "Google Gemini":
            config = load_config("config.yaml")
            api_key, proxy_url = config.get("API_KEY"), config.get("BASE_URL")
            proofread_func = proofread_chapter_with_gemini
            api_args = {'api_key': api_key, 'proxy_url': proxy_url}
        elif api_provider == "OpenAI":
            config = load_config("config_gpt.yaml")
            api_key, base_url = config.get("API_KEY"), config.get("BASE_URL")
            proofread_func = proofread_chapter_with_openai
            api_args = {'api_key': api_key, 'base_url': base_url}
        else:
            yield "é”™è¯¯ï¼šæ— æ•ˆçš„ API æä¾›å•†ã€‚", gr.File(visible=False); return

        if not api_key or "..." in api_key or "YOUR" in api_key:
            yield f"é”™è¯¯: {api_provider} çš„ API_KEY æœªåœ¨é…ç½®æ–‡ä»¶ä¸­æ­£ç¡®é…ç½®ã€‚", gr.File(visible=False)
            return
    except Exception as e:
        yield f"åŠ è½½é…ç½®æ—¶å‡ºé”™: {e}", gr.File(visible=False); return

    chapters_raw = re.split(r'^(ç¬¬.*?ç« )', novel_text, flags=re.MULTILINE)
    chapter_list = []
    if preamble := chapters_raw[0].strip(): chapter_list.append({"title": "ã€åºç« /å‰è¨€ã€‘", "content": preamble})
    for i in range(1, len(chapters_raw), 2):
        if content := chapters_raw[i+1].strip(): chapter_list.append({"title": chapters_raw[i].strip(), "content": content})
    if not chapter_list and novel_text.strip(): chapter_list.append({"title": "ã€å…¨æ–‡ã€‘", "content": novel_text.strip()})
    if not chapter_list: yield "æœªèƒ½è§£æå‡ºä»»ä½•æ–‡æœ¬å†…å®¹ã€‚", gr.File(visible=False); return

    full_report = f"å°è¯´æ ¡å¯¹æŠ¥å‘Š ({prompt_mode} æ¨¡å¼)\n"
    full_report += f"API: {api_provider} | æ¨¡å‹: {model_name} | Temperature: {temperature}\n" # <--- ä¿®æ”¹æŠ¥å‘Šå¤´
    full_report += "====================\n\n"
    yield full_report, gr.File(visible=False)

    for i, chapter in enumerate(chapter_list):
        print(f"[INFO] å¼€å§‹å¤„ç†ç¬¬ {i+1}/{len(chapter_list)} éƒ¨åˆ†: {chapter['title']}")
        status_prefix = f"æ ¡å¯¹ä¸­ ({i+1}/{len(chapter_list)}): {chapter['title']}\n\n"
        yield full_report + status_prefix, gr.File(visible=False)
        result = ""
        try:
            result = proofread_func(
                chapter_title=chapter['title'], chapter_content=chapter['content'],
                model_name=model_name, prompt_template=prompt_template,
                temperature=temperature, **api_args # <--- ä¼ é€’ temperature
            )
        except Exception as e:
            result = f"åœ¨å¤„ç†ç« èŠ‚ '{chapter['title']}' æ—¶ API è°ƒç”¨æœ€ç»ˆå¤±è´¥ã€‚\né”™è¯¯: {e}"
        
        full_report += f"{i+1}/{len(chapter_list)}: {chapter['title']}\n\n"
        full_report += result + "\n\n--------------------\n\n"
        yield full_report, gr.File(visible=False)
        time.sleep(1)

    full_report += "å°è¯´æ ¡å¯¹å®Œæ¯•\n\n"
    print("[INFO] æ‰€æœ‰ç« èŠ‚æ ¡å¯¹å®Œæˆã€‚")
    timestamp = datetime.now().strftime("%Y_%m_%d_%H%M%S")
    output_filename = f"report_{timestamp}.txt"
    with open(output_filename, "w", encoding="utf-8") as f: f.write(full_report)
    yield full_report, gr.File(value=output_filename, visible=True, label=f"ä¸‹è½½æŠ¥å‘Š ({output_filename})")

# --- 4. Gradio ç•Œé¢ (åŠ¨æ€åŠ è½½ prompts, å¢åŠ  temperature æ»‘å—) ---
try:
    models_data = load_models()
    initial_google_models = models_data.get('google', [])
    initial_openai_models = models_data.get('openai', [])
except Exception as e:
    print(f"[FATAL] æ— æ³•åŠ è½½ models.yaml, å°†ä½¿ç”¨é»˜è®¤å€¼ã€‚é”™è¯¯: {e}")
    initial_google_models, initial_openai_models = ["gemini-1.5-flash-latest"], ["gpt-4o"]

# <--- æ–°å¢ï¼šåŠ¨æ€åŠ è½½ prompt æ–‡ä»¶åˆ—è¡¨ ---
def get_prompt_choices():
    prompt_dir = "prompts"
    if not os.path.isdir(prompt_dir):
        return ["error_no_prompt_dir"]
    try:
        # è¿”å›æ–‡ä»¶åï¼ˆä¸å«.txtï¼‰ï¼Œå¹¶æŒ‰å­—æ¯æ’åº
        return sorted([f.replace(".txt", "") for f in os.listdir(prompt_dir) if f.endswith(".txt")])
    except Exception as e:
        print(f"[ERROR] æ— æ³•è¯»å– prompts æ–‡ä»¶å¤¹: {e}")
        return ["error_reading_prompts"]
prompt_choices = get_prompt_choices()

def update_model_choices(provider):
    if provider == "Google Gemini": choices = initial_google_models
    elif provider == "OpenAI": choices = initial_openai_models
    else: choices = []
    return gr.Radio(choices=choices, value=choices[0] if choices else None)

with gr.Blocks(theme=gr.themes.Soft()) as app:
    gr.Markdown("# ğŸ“– å°è¯´æ–‡æœ¬æ™ºèƒ½æ ¡å¯¹å™¨ (å¤š API æ”¯æŒ)")
    gr.Markdown("å°†ä½ çš„å°è¯´æ–‡æœ¬ç²˜è´´åˆ°ä¸‹æ–¹ï¼Œé€‰æ‹© API å’Œæ¨¡å‹ï¼Œå·¥å…·ä¼šè‡ªåŠ¨è¿›è¡Œåˆ†å‰²å’Œæ ¡å¯¹ã€‚")
    
    with gr.Row():
        with gr.Column(scale=2):
            input_text = gr.Textbox(lines=15, label="å°è¯´åŸæ–‡", placeholder="è¯·åœ¨è¿™é‡Œç²˜è´´ä½ çš„å°è¯´å…¨æ–‡...")
            
            gr.Markdown("### æ ¡å¯¹é€‰é¡¹")
            api_provider_selector = gr.Radio(choices=["Google Gemini", "OpenAI"], label="é€‰æ‹© API æä¾›å•†", value="Google Gemini")
            model_selector = gr.Radio(label="é€‰æ‹©æ ¡å¯¹æ¨¡å‹", choices=initial_google_models, value=initial_google_models[0] if initial_google_models else None)
            
            # <--- ä¿®æ”¹ï¼šä½¿ç”¨åŠ¨æ€åŠ è½½çš„ prompt åˆ—è¡¨ ---
            prompt_selector = gr.Radio(
                choices=prompt_choices,
                label="é€‰æ‹©æ ¡å¯¹æ¨¡å¼",
                value=prompt_choices[0] if prompt_choices else None
            )

            # <--- æ–°å¢ï¼šTemperature æ»‘å— ---
            temperature_slider = gr.Slider(
                minimum=0.0, maximum=1.0, step=0.1, value=0.2,
                label="Temperature (å€¼è¶Šä½è¶Šç²¾ç¡®)",
                info="æ§åˆ¶è¾“å‡ºçš„ç¡®å®šæ€§ã€‚æ ¡å¯¹ä»»åŠ¡æ¨è 0.1-0.3"
            )

            submit_btn = gr.Button("ğŸš€ å¼€å§‹æ ¡å¯¹", variant="primary")

        with gr.Column(scale=3):
            output_text = gr.Textbox(lines=28, label="æ ¡å¯¹æŠ¥å‘Š (å®æ—¶æ›´æ–°)", placeholder="æ ¡å¯¹å»ºè®®å°†åœ¨è¿™é‡Œé€ç« æ˜¾ç¤º...", interactive=False)
            download_file = gr.File(interactive=False, visible=False)

    api_provider_selector.change(fn=update_model_choices, inputs=api_provider_selector, outputs=model_selector)
    
    # <--- ä¿®æ”¹ï¼šä¸º click äº‹ä»¶å¢åŠ  temperature_slider è¾“å…¥ ---
    submit_btn.click(
        fn=process_and_proofread_novel,
        inputs=[input_text, api_provider_selector, model_selector, prompt_selector, temperature_slider],
        outputs=[output_text, download_file]
    )

    gr.Markdown("---")
    gr.Markdown("å¼€å‘ by Gemini & AIã€‚")

# --- 5. å¯åŠ¨åº”ç”¨ ---
if __name__ == "__main__":
    required_files = ["config.yaml", "config_gpt.yaml", "models.yaml", "prompts"]
    if all(os.path.exists(f) for f in required_files) and os.path.isdir("prompts") and len(os.listdir("prompts")) > 0:
        print("å¯åŠ¨ Gradio åº”ç”¨... è¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æä¾›çš„ URLã€‚")
        app.launch()
    else:
        print("[FATAL] ç¼ºå°‘å¿…è¦çš„é…ç½®æ–‡ä»¶æˆ– prompts æ–‡ä»¶å¤¹ä¸ºç©ºã€‚è¯·ç¡®ä¿ä»¥ä¸‹å­˜åœ¨ä¸” prompts ä¸ä¸ºç©º:")
        for f in required_files: print(f" - {f}")